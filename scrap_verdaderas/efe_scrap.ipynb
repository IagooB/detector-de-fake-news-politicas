{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL base de la sección de política en Verifica EFE\n",
    "BASE_URL = \"https://verifica.efe.com/politica/page/\"\n",
    "total_pages = 60  # Número de páginas a recorrer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Inicializa el driver de Selenium con configuración para ejecución en modo headless.\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service('chromedriver/chrome.exe')  # Ruta al ejecutable de ChromeDriver\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# Inicializa el driver y herramientas auxiliares\n",
    "driver = initialize_driver()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "# Listas para almacenar los datos recopilados\n",
    "titles, dates, contents, urls = [], [], [], []\n",
    "\n",
    "try:\n",
    "    for page in range(1, total_pages + 1):\n",
    "        # Navegar a la página actual\n",
    "        driver.get(BASE_URL + str(page))\n",
    "        time.sleep(5)  # Esperar a que la página cargue completamente\n",
    "\n",
    "        # Procesar el HTML de la página con BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # Encontrar todos los enlaces a las noticias en la página actual\n",
    "        news_links = soup.find_all(\"a\", class_=\"icons\")\n",
    "\n",
    "        for link in news_links:\n",
    "            # Obtener el enlace de cada noticia\n",
    "            article_url = link.get(\"href\")\n",
    "            urls.append(article_url)\n",
    "\n",
    "            # Navegar a la noticia\n",
    "            driver.get(article_url)\n",
    "            time.sleep(3)  # Esperar a que la noticia cargue\n",
    "\n",
    "            # Procesar el HTML de la noticia\n",
    "            article_source = driver.page_source\n",
    "            article_soup = BeautifulSoup(article_source, \"html.parser\")\n",
    "\n",
    "            # Extraer el título de la noticia\n",
    "            title = article_soup.find(\"h1\", class_=\"sc_layouts_title_caption\").get_text(strip=True)\n",
    "            titles.append(title)\n",
    "\n",
    "            # Extraer la fecha de la noticia\n",
    "            date = article_soup.find(\"span\", class_=\"post_meta_item post_date\").get_text(strip=True)\n",
    "            dates.append(date)\n",
    "\n",
    "            # Extraer el contenido completo de la noticia\n",
    "            paragraphs = article_soup.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "            contents.append(content)\n",
    "\n",
    "            # Volver a la página anterior\n",
    "            driver.back()\n",
    "            time.sleep(2)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Crear un DataFrame con los datos recopilados\n",
    "news_data = pd.DataFrame({\n",
    "    'Título': titles,\n",
    "    'Fecha': dates,\n",
    "    'Contenido': contents,\n",
    "    'URL': urls\n",
    "})\n",
    "\n",
    "# Guardar los datos en un archivo CSV\n",
    "news_data.to_csv(\"noticias_efe.csv\", index=False)\n",
    "print(\"Datos guardados en 'noticias_efe.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
