{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 10\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 20\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 30\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 40\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 50\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 60\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 70\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 80\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 90\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 100\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 110\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 120\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 130\n",
      "Noticia duplicada encontrada: La Fiscalía no pide la misma pena para Laura Borràs que la prevista por asesinato, sino menos de la mitad de la condena mínima, omitiendo...\n",
      "Guardadas las noticias de la página actual: 139\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 149\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 159\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 169\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 179\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 189\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 199\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 209\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 219\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 229\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 239\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 249\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 259\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 269\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 279\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 289\n",
      "Noticias guardadas\n",
      "Guardadas las noticias de la página actual: 294\n",
      "No se pudo encontrar o hacer clic en el botón de la página siguiente. Puede que no haya más páginas disponibles.\n",
      "Ejecución finalizada y datos guardados correctamente.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# URL base de las noticias de fact-check en Verificat\n",
    "BASE_URL = \"https://www.verificat.cat/es/fact-check-politica/\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Configuración del driver de Selenium\n",
    "def initialize_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "driver = initialize_driver()\n",
    "driver.get(BASE_URL)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "# Función para cargar y analizar el contenido de la página\n",
    "\n",
    "def get_page_soup():\n",
    "    page_source = driver.page_source\n",
    "    return BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Función para pasar a la siguiente página\n",
    "\n",
    "def go_to_next_page(current_page):\n",
    "    try:\n",
    "        # Ajuste del selector XPath para encontrar el botón de la página siguiente correctamente\n",
    "        next_page_button = wait.until(EC.element_to_be_clickable((By.XPATH, f\"/html/body/div[1]/div/div/main/article/div[2]/div/div[4]/ul/li/a[text()='{current_page + 1}']\")))\n",
    "        actions.move_to_element(next_page_button).perform()  # Mover el cursor al botón\n",
    "        time.sleep(2)  # Esperar medio segundo\n",
    "        next_page_button.click()  # Hacer clic en el botón\n",
    "        time.sleep(5)  # Esperar 5 segundos para que se cargue la siguiente página\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        print(\"No se pudo encontrar o hacer clic en el botón de la página siguiente. Puede que no haya más páginas disponibles.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Función para extraer detalles de cada noticia\n",
    "\n",
    "def get_article_details(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.content\n",
    "        \n",
    "        if not page_content:\n",
    "            return \"No se encontró el título\", \"No se encontró la fecha\", \"No se encontró el contenido\"\n",
    "\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        # Obtener el título\n",
    "        title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"No se encontró el título\"\n",
    "\n",
    "        # Obtener la fecha\n",
    "        date_tag = soup.find(\"time\")\n",
    "        date = date_tag[\"datetime\"] if date_tag and date_tag.has_attr(\"datetime\") else \"No se encontró la fecha\"\n",
    "\n",
    "        # Obtener el contenido de la noticia\n",
    "        content_elements = soup.select(\"div.entry-content > p\")\n",
    "        content = \" \".join([elem.get_text(strip=True) for elem in content_elements])\n",
    "        content = content if content else \"No se encontró el contenido\"\n",
    "        time.sleep(5)\n",
    "        return title, date, content\n",
    "    except (requests.RequestException, AttributeError) as e:\n",
    "        print(f\"Error al obtener detalles del artículo: {e}\")\n",
    "        return \"No se encontró el título\", \"No se encontró la fecha\", \"No se encontró el contenido\"\n",
    "\n",
    "# Almacenar noticias\n",
    "news_data = []\n",
    "news_counter = 0\n",
    "saved_titles = set()  # Mantener un registro de los títulos ya guardados\n",
    "current_page = 1\n",
    "\n",
    "try:\n",
    "    # Crear el archivo CSV y agregar los encabezados\n",
    "    with open('verificat.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Número de Noticia\", \"Titular\", \"Fecha\", \"Contenido\"])\n",
    "\n",
    "    while current_page <= 30:\n",
    "        # Crear el objeto BeautifulSoup a partir del contenido actual de la página\n",
    "        soup = get_page_soup()\n",
    "\n",
    "        # Obtener los enlaces de las noticias\n",
    "        articles = []\n",
    "        article_cards = soup.select(\"div.ultp-block-content > h3 > a\")\n",
    "        for link in article_cards:\n",
    "            try:\n",
    "                if link and link['href'] not in saved_titles:\n",
    "                    articles.append(link['href'])\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "        if not articles:\n",
    "            print(\"No se encontraron más artículos.\")\n",
    "            break\n",
    "\n",
    "        for article_url in articles:\n",
    "            title, date, content = get_article_details(article_url)\n",
    "\n",
    "            # Verificar si el título ya ha sido guardado\n",
    "            if title in saved_titles:\n",
    "                print(f\"Noticia duplicada encontrada: {title}, omitiendo...\")\n",
    "                continue\n",
    "\n",
    "            news_counter += 1\n",
    "            saved_titles.add(title)\n",
    "            news_data.append([news_counter, title, date, content])\n",
    "\n",
    "            # Cada 10 noticias, guardar en CSV\n",
    "            if news_counter % 10 == 0:\n",
    "                with open('verificat.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerows(news_data)\n",
    "                news_data = []  # Limpiar datos almacenados\n",
    "                print(\"Noticias guardadas\")\n",
    "\n",
    "        # Guardar las noticias después de cada carga\n",
    "        with open('verificat.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(news_data)\n",
    "        news_data = []  # Limpiar datos almacenados\n",
    "        print(f\"Guardadas las noticias de la página actual: {news_counter}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Intentar ir a la siguiente página\n",
    "        if not go_to_next_page(current_page):\n",
    "            break\n",
    "\n",
    "        current_page += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la ejecución: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Guardar las noticias restantes\n",
    "    if news_data:\n",
    "        with open('verificat.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(news_data)\n",
    "\n",
    "    # Cerrar el driver\n",
    "    driver.quit()\n",
    "    print(\"Ejecución finalizada y datos guardados correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
